#!/usr/bin/env python3
"""
æœ¬åœ° 1688 çˆ¬è™«è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
1. ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install playwright supabase python-dotenv
2. ç¡®ä¿å·²å®‰è£…æµè§ˆå™¨: playwright install chromium
3. è¿è¡Œ: python tools/scrape_1688.py "è“ç‰™è€³æœº" --limit 20

è„šæœ¬ä¼šæ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œä½¿ç”¨ä½ å·²ç™»å½•çš„ Chrome profile çˆ¬å– 1688 æœç´¢ç»“æœï¼Œ
ç„¶åå°†æ•°æ®å­˜å…¥ Supabase æ•°æ®åº“ä¾›äº‘ç«¯ API ä½¿ç”¨ã€‚
"""

import argparse
import asyncio
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("è¯·å®‰è£… playwright: pip install playwright && playwright install chromium")
    sys.exit(1)

try:
    from supabase import create_client
except ImportError:
    print("è¯·å®‰è£… supabase: pip install supabase")
    sys.exit(1)

try:
    from dotenv import load_dotenv
except ImportError:
    print("è¯·å®‰è£… python-dotenv: pip install python-dotenv")
    sys.exit(1)


# åŠ è½½ç¯å¢ƒå˜é‡
env_path = Path(__file__).parent.parent / "backend" / ".env"
if env_path.exists():
    load_dotenv(env_path)
else:
    # å°è¯•é¡¹ç›®æ ¹ç›®å½•
    env_path = Path(__file__).parent.parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)


class Local1688Scraper:
    """æœ¬åœ° 1688 çˆ¬è™«ï¼Œä½¿ç”¨ç”¨æˆ·å·²ç™»å½•çš„æµè§ˆå™¨"""

    def __init__(self, headless: bool = False):
        self.headless = headless
        self.supabase = None
        self._init_supabase()

    def _init_supabase(self):
        """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
        url = os.getenv("SUPABASE_URL")
        key = os.getenv("SUPABASE_KEY") or os.getenv("SUPABASE_SERVICE_KEY")

        if not url or not key:
            print("è­¦å‘Š: æœªé…ç½® Supabaseï¼Œæ•°æ®å°†åªè¾“å‡ºåˆ°æ§åˆ¶å°")
            print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® SUPABASE_URL å’Œ SUPABASE_KEY")
            return

        try:
            self.supabase = create_client(url, key)
            print(f"âœ“ Supabase å·²è¿æ¥: {url[:30]}...")
        except Exception as e:
            print(f"Supabase è¿æ¥å¤±è´¥: {e}")

    async def scrape(
        self,
        keyword: str,
        max_price: float = 500,
        limit: int = 20,
    ) -> List[dict]:
        """
        çˆ¬å– 1688 æœç´¢ç»“æœ

        Args:
            keyword: ä¸­æ–‡æœç´¢å…³é”®è¯
            max_price: æœ€é«˜ä»·æ ¼ï¼ˆäººæ°‘å¸ï¼‰
            limit: è·å–æ•°é‡

        Returns:
            ä¾›åº”å•†äº§å“åˆ—è¡¨
        """
        print(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")
        print(f"ğŸ“¦ æœ€é«˜ä»·æ ¼: Â¥{max_price}")
        print(f"ğŸ“Š è·å–æ•°é‡: {limit}")

        async with async_playwright() as p:
            # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼ˆç”¨æˆ·å·²ç™»å½•çš„æµè§ˆå™¨ profileï¼‰
            # è¿™æ ·å¯ä»¥ä½¿ç”¨ç”¨æˆ·çš„ç™»å½•çŠ¶æ€
            user_data_dir = self._get_chrome_profile_path()

            print(f"\nğŸŒ å¯åŠ¨æµè§ˆå™¨...")
            if user_data_dir and Path(user_data_dir).exists():
                print(f"   ä½¿ç”¨ Chrome Profile: {user_data_dir}")
                context = await p.chromium.launch_persistent_context(
                    user_data_dir,
                    headless=self.headless,
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-CN",
                )
            else:
                print("   ä½¿ç”¨æ–°çš„æµè§ˆå™¨å®ä¾‹ï¼ˆéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰")
                browser = await p.chromium.launch(headless=self.headless)
                context = await browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-CN",
                )

            page = await context.new_page()

            try:
                # æ„å»ºæœç´¢ URL
                search_url = f"https://s.1688.com/selloffer/offer_search.htm?keywords={keyword}"
                if max_price > 0:
                    search_url += f"&e_price={int(max_price)}"

                print(f"\nğŸ“„ è®¿é—®: {search_url}")
                await page.goto(search_url, wait_until="networkidle", timeout=60000)

                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(3)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•æˆ–éªŒè¯
                page_title = await page.title()
                print(f"   é¡µé¢æ ‡é¢˜: {page_title}")

                if "éªŒè¯" in page_title or "ç™»å½•" in page_title:
                    print("\nâš ï¸  æ£€æµ‹åˆ°éªŒè¯ç æˆ–ç™»å½•é¡µé¢")
                    print("   è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯/ç™»å½•ï¼Œç„¶åæŒ‰ Enter ç»§ç»­...")
                    input()
                    await asyncio.sleep(2)

                # æå–äº§å“æ•°æ®
                suppliers = await self._extract_products(page, limit)

                print(f"\nâœ“ æå–åˆ° {len(suppliers)} ä¸ªäº§å“")

                # ä¿å­˜åˆ°æ•°æ®åº“
                if suppliers and self.supabase:
                    await self._save_to_database(suppliers, keyword)

                return suppliers

            except Exception as e:
                print(f"\nâŒ çˆ¬å–é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return []

            finally:
                await context.close()

    async def _extract_products(self, page, limit: int) -> List[dict]:
        """ä»é¡µé¢æå–äº§å“æ•°æ®"""
        products = []

        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        selectors = [
            ".search-offer-item",
            ".sm-offer-item",
            ".offer-list-row",
            "[data-offer-id]",
        ]

        items = []
        for selector in selectors:
            items = await page.query_selector_all(selector)
            if items:
                print(f"   ä½¿ç”¨é€‰æ‹©å™¨: {selector} (æ‰¾åˆ° {len(items)} ä¸ª)")
                break

        if not items:
            print("   âš ï¸ æœªæ‰¾åˆ°äº§å“å…ƒç´ ï¼Œå°è¯•ä»é¡µé¢ JSON æå–...")
            return await self._extract_from_json(page, limit)

        for i, item in enumerate(items[:limit]):
            try:
                product = await self._parse_item(item)
                if product:
                    products.append(product)
                    print(f"   [{i+1}] {product['title'][:30]}... Â¥{product['price']}")
            except Exception as e:
                print(f"   [{i+1}] è§£æå¤±è´¥: {e}")

        return products

    async def _parse_item(self, item) -> Optional[dict]:
        """è§£æå•ä¸ªäº§å“å…ƒç´ """
        # æå–æ ‡é¢˜
        title_elem = await item.query_selector(
            ".title-text, .title, .offer-title, h2 a, [class*='title'] a"
        )
        title = await title_elem.inner_text() if title_elem else ""
        title = title.strip()

        if not title:
            return None

        # æå–ä»·æ ¼
        price_elem = await item.query_selector("[class*='price'], .price, .offer-price")
        price_text = await price_elem.inner_text() if price_elem else "0"
        price = self._parse_price(price_text)

        # æå–é“¾æ¥å’Œ offer_id
        link_elem = await item.query_selector(
            "a[href*='detail.1688.com'], a[href*='/offer/']"
        )
        url = await link_elem.get_attribute("href") if link_elem else ""
        if url and not url.startswith("http"):
            url = f"https:{url}" if url.startswith("//") else f"https://detail.1688.com{url}"

        offer_id = self._extract_offer_id(url)

        # è·³è¿‡éäº§å“é“¾æ¥
        if "similar_search" in url or not offer_id:
            return None

        # æå–å›¾ç‰‡
        img_elem = await item.query_selector("img")
        image_url = await img_elem.get_attribute("src") if img_elem else None
        if image_url and not image_url.startswith("http"):
            image_url = f"https:{image_url}"

        # æå–é”€é‡
        sold_elem = await item.query_selector("[class*='sold'], [class*='deal'], .trade-quantity")
        sold_text = await sold_elem.inner_text() if sold_elem else "0"
        sold_count = self._parse_sold_count(sold_text)

        # æå–ä¾›åº”å•†
        supplier_elem = await item.query_selector(".company-name, .seller-name, [class*='company'] a")
        supplier_name = await supplier_elem.inner_text() if supplier_elem else "Unknown"

        # æå–ä½ç½®
        location_elem = await item.query_selector(".location, .address, [class*='location']")
        location = await location_elem.inner_text() if location_elem else None

        return {
            "offer_id": offer_id,
            "title": title,
            "price": price,
            "product_url": url,
            "image_url": image_url,
            "sold_count": sold_count,
            "supplier_name": supplier_name.strip(),
            "location": location.strip() if location else None,
            "scraped_at": datetime.now().isoformat(),
        }

    async def _extract_from_json(self, page, limit: int) -> List[dict]:
        """ä»é¡µé¢ JSON æ•°æ®æå–"""
        try:
            script_content = await page.evaluate("""
                () => {
                    const scripts = document.querySelectorAll('script');
                    for (const script of scripts) {
                        if (script.textContent && script.textContent.includes('offerList')) {
                            return script.textContent;
                        }
                    }
                    return null;
                }
            """)

            if not script_content:
                return []

            # è§£æ JSON
            match = re.search(r'"offerList"\s*:\s*(\[.*?\])', script_content, re.DOTALL)
            if not match:
                return []

            offers = json.loads(match.group(1))
            products = []

            for offer in offers[:limit]:
                products.append({
                    "offer_id": str(offer.get("offerId", offer.get("id", ""))),
                    "title": offer.get("subject", offer.get("title", "")),
                    "price": float(offer.get("price", 0)),
                    "product_url": f"https://detail.1688.com/offer/{offer.get('offerId', '')}.html",
                    "image_url": offer.get("imageUrl"),
                    "sold_count": int(offer.get("gmvReTrade30Day", 0)),
                    "supplier_name": offer.get("companyName", "Unknown"),
                    "location": offer.get("location"),
                    "scraped_at": datetime.now().isoformat(),
                })

            return products

        except Exception as e:
            print(f"   JSON æå–å¤±è´¥: {e}")
            return []

    def _parse_price(self, price_text: str) -> float:
        """è§£æä»·æ ¼æ–‡æœ¬"""
        price_text = re.sub(r"[^\d.]", "", price_text)
        try:
            return float(price_text) if price_text else 0
        except ValueError:
            return 0

    def _parse_sold_count(self, sold_text: str) -> int:
        """è§£æé”€é‡æ–‡æœ¬"""
        sold_text = sold_text.strip()
        if "ä¸‡" in sold_text:
            num = re.search(r"([\d.]+)", sold_text)
            if num:
                return int(float(num.group(1)) * 10000)
        num = re.search(r"(\d+)", sold_text)
        return int(num.group(1)) if num else 0

    def _extract_offer_id(self, url: str) -> str:
        """ä» URL æå– offer ID"""
        match = re.search(r"/offer/(\d+)", url)
        if match:
            return match.group(1)
        match = re.search(r"offerId[=:](\d+)", url)
        if match:
            return match.group(1)
        # å°è¯•æå–ä»»ä½•é•¿æ•°å­—
        match = re.search(r"(\d{10,})", url)
        if match:
            return match.group(1)
        return ""

    def _get_chrome_profile_path(self) -> Optional[str]:
        """è·å– Chrome ç”¨æˆ·æ•°æ®ç›®å½•"""
        import platform

        system = platform.system()
        home = Path.home()

        if system == "Darwin":  # macOS
            return str(home / "Library/Application Support/Google/Chrome")
        elif system == "Windows":
            return str(home / "AppData/Local/Google/Chrome/User Data")
        elif system == "Linux":
            return str(home / ".config/google-chrome")

        return None

    async def _save_to_database(self, products: List[dict], keyword: str):
        """ä¿å­˜åˆ° Supabase æ•°æ®åº“"""
        if not self.supabase:
            return

        print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")

        for product in products:
            try:
                # æ·»åŠ æœç´¢å…³é”®è¯
                product["search_keyword"] = keyword

                # Upsertï¼ˆæ’å…¥æˆ–æ›´æ–°ï¼‰
                self.supabase.table("suppliers_1688").upsert(
                    product,
                    on_conflict="offer_id"
                ).execute()

            except Exception as e:
                print(f"   ä¿å­˜å¤±è´¥ [{product['offer_id']}]: {e}")

        print(f"âœ“ å·²ä¿å­˜ {len(products)} æ¡è®°å½•")


async def main():
    parser = argparse.ArgumentParser(description="æœ¬åœ° 1688 çˆ¬è™«")
    parser.add_argument("keyword", help="æœç´¢å…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰")
    parser.add_argument("--limit", type=int, default=20, help="è·å–æ•°é‡ (é»˜è®¤: 20)")
    parser.add_argument("--max-price", type=float, default=500, help="æœ€é«˜ä»·æ ¼ (é»˜è®¤: 500)")
    parser.add_argument("--headless", action="store_true", help="æ— å¤´æ¨¡å¼è¿è¡Œ")
    parser.add_argument("--output", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    scraper = Local1688Scraper(headless=args.headless)
    products = await scraper.scrape(
        keyword=args.keyword,
        max_price=args.max_price,
        limit=args.limit,
    )

    # è¾“å‡ºåˆ°æ–‡ä»¶
    if args.output and products:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ å·²ä¿å­˜åˆ°: {args.output}")

    # æ‰“å°æ‘˜è¦
    if products:
        print(f"\n{'='*50}")
        print(f"æœç´¢å…³é”®è¯: {args.keyword}")
        print(f"è·å–äº§å“æ•°: {len(products)}")
        print(f"ä»·æ ¼èŒƒå›´: Â¥{min(p['price'] for p in products):.2f} - Â¥{max(p['price'] for p in products):.2f}")
        print(f"{'='*50}")


if __name__ == "__main__":
    asyncio.run(main())
