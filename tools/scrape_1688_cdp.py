#!/usr/bin/env python3
"""
1688 çˆ¬è™« - è¿æ¥å·²æ‰“å¼€çš„ Chrome æµè§ˆå™¨

ä½¿ç”¨æ–¹æ³•:
1. å…ˆå…³é—­æ‰€æœ‰ Chrome çª—å£
2. ç”¨è°ƒè¯•æ¨¡å¼å¯åŠ¨ Chrome:
   /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222
3. åœ¨ Chrome ä¸­ç™»å½• 1688ï¼Œç„¶åæ‰“å¼€æœç´¢é¡µé¢
4. è¿è¡Œæ­¤è„šæœ¬:
   python tools/scrape_1688_cdp.py

è„šæœ¬ä¼šè¿æ¥åˆ°ä½ å·²æ‰“å¼€çš„ Chromeï¼Œæå–å½“å‰é¡µé¢çš„äº§å“æ•°æ®ã€‚
"""

import argparse
import asyncio
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional

sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

try:
    from playwright.async_api import async_playwright
except ImportError:
    print("è¯·å®‰è£… playwright: pip install playwright")
    sys.exit(1)

try:
    from supabase import create_client
except ImportError:
    create_client = None

try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent.parent / "backend" / ".env"
    if env_path.exists():
        load_dotenv(env_path)
except ImportError:
    pass


class ChromeCDPScraper:
    """è¿æ¥å·²æ‰“å¼€çš„ Chrome æµè§ˆå™¨è¿›è¡Œçˆ¬å–"""

    def __init__(self, cdp_url: str = "http://localhost:9222"):
        self.cdp_url = cdp_url
        self.supabase = None
        self._init_supabase()

    def _init_supabase(self):
        """åˆå§‹åŒ– Supabase"""
        if not create_client:
            return
        url = os.getenv("SUPABASE_URL")
        key = os.getenv("SUPABASE_KEY") or os.getenv("SUPABASE_SERVICE_KEY")
        if url and key:
            try:
                self.supabase = create_client(url, key)
                print(f"âœ“ Supabase å·²è¿æ¥")
            except Exception as e:
                print(f"Supabase è¿æ¥å¤±è´¥: {e}")

    async def scrape_current_page(self, keyword: str = None) -> List[dict]:
        """ä»å½“å‰æ‰“å¼€çš„ 1688 é¡µé¢æå–æ•°æ®"""
        print(f"\nğŸ”— è¿æ¥åˆ° Chrome: {self.cdp_url}")

        async with async_playwright() as p:
            try:
                browser = await p.chromium.connect_over_cdp(self.cdp_url)
                print("âœ“ å·²è¿æ¥åˆ° Chrome")
            except Exception as e:
                print(f"\nâŒ æ— æ³•è¿æ¥åˆ° Chrome: {e}")
                print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œ:")
                print("1. å…³é—­æ‰€æœ‰ Chrome çª—å£")
                print("2. è¿è¡Œ: /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222")
                print("3. åœ¨ Chrome ä¸­æ‰“å¼€ 1688 æœç´¢é¡µé¢")
                print("4. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
                return []

            contexts = browser.contexts
            if not contexts:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æµè§ˆå™¨ä¸Šä¸‹æ–‡")
                return []

            # è·å–ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­çš„é¡µé¢
            pages = contexts[0].pages
            target_page = None

            for page in pages:
                url = page.url
                if "1688.com" in url:
                    target_page = page
                    print(f"âœ“ æ‰¾åˆ° 1688 é¡µé¢: {url[:60]}...")
                    break

            if not target_page:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„ 1688 é¡µé¢")
                print("   è¯·åœ¨ Chrome ä¸­æ‰“å¼€ 1688 æœç´¢ç»“æœé¡µé¢")
                return []

            # è·å–é¡µé¢æ ‡é¢˜
            title = await target_page.title()
            print(f"   é¡µé¢æ ‡é¢˜: {title}")

            # ä» URL æå–å…³é”®è¯
            if not keyword:
                url = target_page.url
                match = re.search(r'keywords=([^&]+)', url)
                if match:
                    from urllib.parse import unquote
                    keyword = unquote(match.group(1))
                    print(f"   æœç´¢å…³é”®è¯: {keyword}")

            # æå–äº§å“
            products = await self._extract_products(target_page, keyword)
            print(f"\nâœ“ æå–åˆ° {len(products)} ä¸ªäº§å“")

            # ä¿å­˜åˆ°æ•°æ®åº“
            if products and self.supabase and keyword:
                await self._save_to_database(products, keyword)

            return products

    async def _extract_products(self, page, keyword: str) -> List[dict]:
        """æå–äº§å“æ•°æ®"""
        products = []

        # å°è¯•ä¸åŒé€‰æ‹©å™¨
        selectors = [
            ".search-offer-item",
            ".sm-offer-item",
            ".offer-list-row",
            "[data-offer-id]",
        ]

        items = []
        for selector in selectors:
            items = await page.query_selector_all(selector)
            if items:
                print(f"   ä½¿ç”¨é€‰æ‹©å™¨: {selector} (æ‰¾åˆ° {len(items)} ä¸ª)")
                break

        if not items:
            print("   âš ï¸ æœªæ‰¾åˆ°äº§å“å…ƒç´ ")
            return []

        for i, item in enumerate(items[:50]):  # æœ€å¤š 50 ä¸ª
            try:
                product = await self._parse_item(item, keyword)
                if product:
                    products.append(product)
                    if i < 5:  # åªæ˜¾ç¤ºå‰ 5 ä¸ª
                        print(f"   [{i+1}] {product['title'][:30]}... Â¥{product['price']}")
            except Exception as e:
                pass

        if len(products) > 5:
            print(f"   ... è¿˜æœ‰ {len(products) - 5} ä¸ªäº§å“")

        return products

    async def _parse_item(self, item, keyword: str) -> Optional[dict]:
        """è§£æå•ä¸ªäº§å“"""
        # æ ‡é¢˜
        title_elem = await item.query_selector(".title-text, .title, .offer-title, h2 a, [class*='title'] a")
        title = await title_elem.inner_text() if title_elem else ""
        title = title.strip()
        if not title:
            return None

        # ä»·æ ¼
        price_elem = await item.query_selector("[class*='price'], .price, .offer-price")
        price_text = await price_elem.inner_text() if price_elem else "0"
        price = self._parse_price(price_text)

        # é“¾æ¥
        link_elem = await item.query_selector("a[href*='detail.1688.com'], a[href*='/offer/']")
        url = await link_elem.get_attribute("href") if link_elem else ""
        if url and not url.startswith("http"):
            url = f"https:{url}" if url.startswith("//") else f"https://detail.1688.com{url}"

        # è·³è¿‡éäº§å“é“¾æ¥
        if "similar_search" in url:
            return None

        offer_id = self._extract_offer_id(url)
        if not offer_id:
            return None

        # å›¾ç‰‡
        img_elem = await item.query_selector("img")
        image_url = await img_elem.get_attribute("src") if img_elem else None
        if image_url and not image_url.startswith("http"):
            image_url = f"https:{image_url}"

        # é”€é‡
        sold_elem = await item.query_selector("[class*='sold'], [class*='deal'], .trade-quantity")
        sold_text = await sold_elem.inner_text() if sold_elem else "0"
        sold_count = self._parse_sold_count(sold_text)

        # ä¾›åº”å•†
        supplier_elem = await item.query_selector(".company-name, .seller-name, [class*='company'] a")
        supplier_name = await supplier_elem.inner_text() if supplier_elem else "Unknown"

        # ä½ç½®
        location_elem = await item.query_selector(".location, .address, [class*='location']")
        location = await location_elem.inner_text() if location_elem else None

        return {
            "offer_id": offer_id,
            "title": title,
            "price": price,
            "product_url": url,
            "image_url": image_url,
            "sold_count": sold_count,
            "supplier_name": supplier_name.strip(),
            "location": location.strip() if location else None,
            "search_keyword": keyword,
            "scraped_at": datetime.now().isoformat(),
        }

    def _parse_price(self, text: str) -> float:
        text = re.sub(r"[^\d.]", "", text)
        try:
            return float(text) if text else 0
        except ValueError:
            return 0

    def _parse_sold_count(self, text: str) -> int:
        text = text.strip()
        if "ä¸‡" in text:
            num = re.search(r"([\d.]+)", text)
            if num:
                return int(float(num.group(1)) * 10000)
        num = re.search(r"(\d+)", text)
        return int(num.group(1)) if num else 0

    def _extract_offer_id(self, url: str) -> str:
        match = re.search(r"/offer/(\d+)", url)
        if match:
            return match.group(1)
        match = re.search(r"(\d{10,})", url)
        if match:
            return match.group(1)
        return ""

    async def _save_to_database(self, products: List[dict], keyword: str):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        if not self.supabase:
            return

        print(f"\nğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“...")
        saved = 0
        for product in products:
            try:
                self.supabase.table("suppliers_1688").upsert(
                    product,
                    on_conflict="offer_id"
                ).execute()
                saved += 1
            except Exception as e:
                print(f"   ä¿å­˜å¤±è´¥: {e}")

        print(f"âœ“ å·²ä¿å­˜ {saved} æ¡è®°å½•")


async def main():
    parser = argparse.ArgumentParser(description="ä»å·²æ‰“å¼€çš„ Chrome æå– 1688 æ•°æ®")
    parser.add_argument("--keyword", help="æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨ä» URL æå–ï¼‰")
    parser.add_argument("--port", type=int, default=9222, help="Chrome è°ƒè¯•ç«¯å£ (é»˜è®¤: 9222)")
    parser.add_argument("--output", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    scraper = ChromeCDPScraper(cdp_url=f"http://localhost:{args.port}")
    products = await scraper.scrape_current_page(keyword=args.keyword)

    if args.output and products:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ å·²ä¿å­˜åˆ°: {args.output}")

    if products:
        print(f"\n{'='*50}")
        print(f"æ€»è®¡: {len(products)} ä¸ªäº§å“")
        if products:
            prices = [p['price'] for p in products if p['price'] > 0]
            if prices:
                print(f"ä»·æ ¼èŒƒå›´: Â¥{min(prices):.2f} - Â¥{max(prices):.2f}")
        print(f"{'='*50}")


if __name__ == "__main__":
    asyncio.run(main())
